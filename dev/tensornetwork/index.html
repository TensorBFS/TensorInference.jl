<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Tensor Networks · TensorInference.jl</title><script data-outdated-warner src="../assets/warner.js"></script><link rel="canonical" href="https://TensorBFS.github.io/TensorInference.jl/tensornetwork/"/><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script><link href="../assets/favicon.ico" rel="icon" type="image/x-icon"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img src="../assets/logo.svg" alt="TensorInference.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../">TensorInference.jl</a></span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><span class="tocitem">Background</span><ul><li><a class="tocitem" href="../probabilisticinference/">Probabilistic Inference</a></li><li class="is-active"><a class="tocitem" href>Tensor Networks</a><ul class="internal"><li><a class="tocitem" href="#What-is-a-tensor?"><span>What is a tensor?</span></a></li><li><a class="tocitem" href="#What-is-a-tensor-network?"><span>What is a tensor network?</span></a></li><li><a class="tocitem" href="#Tensor-network-contraction-orders"><span>Tensor network contraction orders</span></a></li><li><a class="tocitem" href="#References"><span>References</span></a></li></ul></li><li><a class="tocitem" href="../uai-file-formats/">UAI file formats</a></li></ul></li><li><span class="tocitem">Examples</span><ul><li><a class="tocitem" href="../examples-overview/">Overview</a></li><li><a class="tocitem" href="../generated/asia-network/main/">Asia Network</a></li><li><a class="tocitem" href="../generated/hard-core-lattice-gas/main/">Hard-core Lattice Gas</a></li></ul></li><li><a class="tocitem" href="../generated/performance/">Performance tips</a></li><li><span class="tocitem">API</span><ul><li><a class="tocitem" href="../api/public/">Public</a></li><li><a class="tocitem" href="../api/internal/">Internal</a></li></ul></li><li><a class="tocitem" href="../contributing/">Contributing</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Background</a></li><li class="is-active"><a href>Tensor Networks</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Tensor Networks</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/TensorBFS/TensorInference.jl/blob/main/docs/src/tensornetwork.md#" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Tensor-networks"><a class="docs-heading-anchor" href="#Tensor-networks">Tensor networks</a><a id="Tensor-networks-1"></a><a class="docs-heading-anchor-permalink" href="#Tensor-networks" title="Permalink"></a></h1><p>We now introduce the core ideas of tensor networks, highlighting their connections with probabilistic graphical models (PGM) to align the terminology between them.</p><p>For our purposes, a tensor is equivalent to the concept of a factor as defined in the PGM domain, which we detail more formally below.</p><h2 id="What-is-a-tensor?"><a class="docs-heading-anchor" href="#What-is-a-tensor?">What is a tensor?</a><a id="What-is-a-tensor?-1"></a><a class="docs-heading-anchor-permalink" href="#What-is-a-tensor?" title="Permalink"></a></h2><p><em>Definition</em>: A tensor <span>$T$</span> is defined as:</p><p class="math-container">\[T: \prod_{V \in \bm{V}} \mathcal{D}_{V} \rightarrow \texttt{number}.\]</p><p>Here, the function <span>$T$</span> maps each possible instantiation of the random variables in its scope <span>$\bm{V}$</span> to a generic number type. In the context of tensor networks, a minimum requirement is that the number type is a commutative semiring. To define a commutative semiring with the addition operation <span>$\oplus$</span> and the multiplication operation <span>$\odot$</span> on a set <span>$S$</span>, the following relations must hold for any arbitrary three elements <span>$a, b, c \in S$</span>.</p><p class="math-container">\[\newcommand{\mymathbb}[1]{\mathbb{#1}}
\begin{align*}
(a \oplus b) \oplus c = a \oplus (b \oplus c) &amp; \hspace{5em}\text{$\triangleright$ commutative monoid $\oplus$ with identity $\mymathbb{0}$}\\
a \oplus \mymathbb{0} = \mymathbb{0} \oplus a = a &amp;\\
a \oplus b = b \oplus a &amp;\\
&amp;\\
(a \odot b) \odot c = a \odot (b \odot c)  &amp;   \hspace{5em}\text{$\triangleright$ commutative monoid $\odot$ with identity $\mymathbb{1}$}\\
a \odot  \mymathbb{1} =  \mymathbb{1} \odot a = a &amp;\\
a \odot b = b \odot a &amp;\\
&amp;\\
a \odot (b\oplus c) = a\odot b \oplus a\odot c  &amp;  \hspace{5em}\text{$\triangleright$ left and right distributive}\\
(a\oplus b) \odot c = a\odot c \oplus b\odot c &amp;\\
&amp;\\
a \odot \mymathbb{0} = \mymathbb{0} \odot a = \mymathbb{0}
\end{align*}\]</p><p>Tensors are represented using multidimensional arrays of nonnegative numbers with labeled dimensions. These labels correspond to the array&#39;s indices, which in turn represent the set of random variables that the tensor is a function of. Thus, in this context, the terms <strong>label</strong>, <strong>index</strong>, and <strong>variable</strong> are synonymous and hence used interchangeably.</p><h2 id="What-is-a-tensor-network?"><a class="docs-heading-anchor" href="#What-is-a-tensor-network?">What is a tensor network?</a><a id="What-is-a-tensor-network?-1"></a><a class="docs-heading-anchor-permalink" href="#What-is-a-tensor-network?" title="Permalink"></a></h2><p>We now turn our attention to defining a <strong>tensor network</strong>, a mathematical object used to represent a multilinear map between tensors. This concept is widely employed in fields like condensed matter physics <sup class="footnote-reference"><a id="citeref-Orus2014" href="#footnote-Orus2014">[Orus2014]</a></sup><sup class="footnote-reference"><a id="citeref-Pfeifer2014" href="#footnote-Pfeifer2014">[Pfeifer2014]</a></sup>, quantum simulation <sup class="footnote-reference"><a id="citeref-Markov2008" href="#footnote-Markov2008">[Markov2008]</a></sup><sup class="footnote-reference"><a id="citeref-Pan2022" href="#footnote-Pan2022">[Pan2022]</a></sup>, and even in solving combinatorial optimization problems <sup class="footnote-reference"><a id="citeref-Liu2023" href="#footnote-Liu2023">[Liu2023]</a></sup>. It&#39;s worth noting that we use a generalized version of the conventional notation, most commonly known through the <a href="https://numpy.org/doc/stable/reference/generated/numpy.einsum.html">eisnum</a> function, which is commonly used in high-performance computing. Packages that implement this conventional notation include</p><ul><li><a href="https://numpy.org/doc/stable/reference/generated/numpy.einsum.html">numpy</a></li><li><a href="https://github.com/under-Peter/OMEinsum.jl">OMEinsum.jl</a></li><li><a href="https://pytorch.org/docs/stable/generated/torch.einsum.html">PyTorch</a></li><li><a href="https://www.tensorflow.org/api_docs/python/tf/einsum">TensorFlow</a></li></ul><p>This approach allows us to represent a broader range of sum-product multilinear operations between tensors, thus meeting the requirements of the PGM field.</p><p><em>Definition</em><sup class="footnote-reference"><a id="citeref-Liu2023" href="#footnote-Liu2023">[Liu2023]</a></sup>: A tensor network is a multilinear map represented by the triple <span>$\mathcal{N} = (\Lambda, \mathcal{T}, \bm{\sigma}_0)$</span>, where:</p><ul><li><span>$\Lambda$</span> is the set of variables present in the network   <span>$\mathcal{N}$</span>.</li><li><span>$\mathcal{T} = \{ T^{(k)}_{\bm{\sigma}_k} \}_{k=1}^{M}$</span> is the set of   input tensors, where each tensor <span>$T^{(k)}_{\bm{\sigma}_k}$</span> is identified   by a superscript <span>$(k)$</span> and has an associated scope <span>$\bm{\sigma}_k$</span>.</li><li><span>$\bm{\sigma}_0$</span> specifies the scope of the output tensor.</li></ul><p>More specifically, each tensor <span>$T^{(k)}_{\bm{\sigma}_k} \in \mathcal{T}$</span> is labeled by a string <span>$\bm{\sigma}_k \in \Lambda^{r \left(T^{(k)} \right)}$</span>, where <span>$r \left(T^{(k)} \right)$</span> is the rank of <span>$T^{(k)}$</span>. The multilinear map, also known as the <code>contraction</code>, applied to this triple is defined as</p><p class="math-container">\[\texttt{contract}(\Lambda, \mathcal{T}, \bm{\sigma}_0) = \sum_{\bm{\sigma}_{\Lambda
\setminus [\bm{\sigma}_0]}} \prod_{k=1}^{M} T^{(k)}_{\bm{\sigma}_k},\]</p><p>Notably, the summation extends over all instantiations of the variables that are not part of the output tensor.</p><p>As an example, consider matrix multiplication, which can be specified as a tensor network contraction:</p><p class="math-container">\[  (AB)_{ik} = \texttt{contract}\left(\{i,j,k\}, \{A_{ij}, B_{jk}\}, ik\right),\]</p><p>Here, matrices <span>$A$</span> and <span>$B$</span> are input tensors labeled by strings <span>$ij, jk \in \{i, j, k\}^2$</span>. The output tensor is labeled by string <span>$ik$</span>. Summations run over indices <span>$\Lambda \setminus [ik] = \{j\}$</span>. The contraction corresponds to</p><p class="math-container">\[  \texttt{contract}\left(\{i,j,k\}, \{A_{ij}, B_{jk}\}, ik\right) = \sum_j
  A_{ij}B_{jk},\]</p><p>In the einsum notation commonly used in various programming languages, this is equivalent to <code>ij, jk -&gt; ik</code>.</p><p>Diagrammatically, a tensor network can be represented as an <em>open hypergraph</em>. In this diagram, a tensor maps to a vertex, and a variable maps to a hyperedge. Tensors sharing the same variable are connected by the same hyperedge for that variable. The diagrammatic representation of matrix multiplication is:</p><img src="the-tensor-network1.svg"  style="margin-left: auto; margin-right: auto; display:block; width=50%"><p>In this diagram, we use different colors to denote different hyperedges. Hyperedges for <span>$i$</span> and <span>$j$</span> are left open to denote variables in the output string <span>$\bm{\sigma}_0$</span>. The reason we use hyperedges rather than regular edges will become clear in the following star contraction example.</p><p class="math-container">\[  \texttt{contract}(\{i,j,k,l\}, \{A_{il}, B_{jl}, C_{kl}\}, ijk) = \sum_{l}A_{il}
  B_{jl} C_{kl}\]</p><p>The equivalent einsum notation employed by many programming languages is <code>il, jl, kl -&gt; ijk</code>.</p><p>Since the variable <span>$l$</span> is shared across all three tensors, a simple graph can&#39;t capture the diagram&#39;s complexity. The more appropriate hypergraph representation is shown below.</p><img src="the-tensor-network2.svg"  style="margin-left: auto; margin-right: auto; display:block; width=50%"><p>As a final note, our definition of a tensor network allows for repeated indices within the same tensor, which translates to self-loops in their corresponding diagrams.</p><h2 id="Tensor-network-contraction-orders"><a class="docs-heading-anchor" href="#Tensor-network-contraction-orders">Tensor network contraction orders</a><a id="Tensor-network-contraction-orders-1"></a><a class="docs-heading-anchor-permalink" href="#Tensor-network-contraction-orders" title="Permalink"></a></h2><p>The performance of a tensor network contraction depends on the order in which the tensors are contracted. The order of contraction is usually specified by binary trees, where the leaves are the input tensors and the internal nodes represent the order of contraction. The root of the tree is the output tensor.</p><p>Numerous approaches have been proposed to determine efficient contraction orderings, which include:</p><ul><li>Greedy algorithms</li><li>Breadth-first search and Dynamic programming <sup class="footnote-reference"><a id="citeref-Pfeifer2014" href="#footnote-Pfeifer2014">[Pfeifer2014]</a></sup></li><li>Graph bipartitioning <sup class="footnote-reference"><a id="citeref-Gray2021" href="#footnote-Gray2021">[Gray2021]</a></sup></li><li>Local search <sup class="footnote-reference"><a id="citeref-Kalachev2021" href="#footnote-Kalachev2021">[Kalachev2021]</a></sup></li></ul><p>Some of these have been implemented in the <a href="https://github.com/under-Peter/OMEinsum.jl">OMEinsum</a> package. Please check <a href="../generated/performance/#Performance-Tips">Performance Tips</a> for more details.</p><h2 id="References"><a class="docs-heading-anchor" href="#References">References</a><a id="References-1"></a><a class="docs-heading-anchor-permalink" href="#References" title="Permalink"></a></h2><section class="footnotes is-size-7"><ul><li class="footnote" id="footnote-Orus2014"><a class="tag is-link" href="#citeref-Orus2014">Orus2014</a>Orús R. A practical introduction to tensor networks: Matrix product states and projected entangled pair states[J]. Annals of physics, 2014, 349: 117-158.</li><li class="footnote" id="footnote-Markov2008"><a class="tag is-link" href="#citeref-Markov2008">Markov2008</a>Markov I L, Shi Y. Simulating quantum computation by contracting tensor networks[J]. SIAM Journal on Computing, 2008, 38(3): 963-981.</li><li class="footnote" id="footnote-Pfeifer2014"><a class="tag is-link" href="#citeref-Pfeifer2014">Pfeifer2014</a>Pfeifer R N C, Haegeman J, Verstraete F. Faster identification of optimal contraction sequences for tensor networks[J]. Physical Review E, 2014, 90(3): 033315.</li><li class="footnote" id="footnote-Gray2021"><a class="tag is-link" href="#citeref-Gray2021">Gray2021</a>Gray J, Kourtis S. Hyper-optimized tensor network contraction[J]. Quantum, 2021, 5: 410.</li><li class="footnote" id="footnote-Kalachev2021"><a class="tag is-link" href="#citeref-Kalachev2021">Kalachev2021</a>Kalachev G, Panteleev P, Yung M H. Multi-tensor contraction for XEB verification of quantum circuits[J]. arXiv:2108.05665, 2021.</li><li class="footnote" id="footnote-Pan2022"><a class="tag is-link" href="#citeref-Pan2022">Pan2022</a>Pan F, Chen K, Zhang P. Solving the sampling problem of the sycamore quantum circuits[J]. Physical Review Letters, 2022, 129(9): 090502.</li><li class="footnote" id="footnote-Liu2023"><a class="tag is-link" href="#citeref-Liu2023">Liu2023</a>Liu J G, Gao X, Cain M, et al. Computing solution space properties of combinatorial optimization problems via generic tensor networks[J]. SIAM Journal on Scientific Computing, 2023, 45(3): A1239-A1270.</li></ul></section></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../probabilisticinference/">« Probabilistic Inference</a><a class="docs-footer-nextpage" href="../uai-file-formats/">UAI file formats »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.25 on <span class="colophon-date" title="Monday 11 September 2023 16:09">Monday 11 September 2023</span>. Using Julia version 1.9.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
