<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Performance tips · TensorInference.jl</title><script data-outdated-warner src="../../assets/warner.js"></script><link rel="canonical" href="https://TensorBFS.github.io/TensorInference.jl/generated/performance/"/><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="../../assets/favicon.ico" rel="icon" type="image/x-icon"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.svg" alt="TensorInference.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../">TensorInference.jl</a></span></div><form class="docs-search" action="../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><a class="tocitem" href="../../background/">Background</a></li><li><span class="tocitem">Examples</span><ul><li><a class="tocitem" href="../../examples-overview/">Overview</a></li><li><a class="tocitem" href="../asia/main/">Asia network</a></li></ul></li><li><a class="tocitem" href="../../uai-file-formats/">UAI file formats</a></li><li class="is-active"><a class="tocitem" href>Performance tips</a><ul class="internal"><li><a class="tocitem" href="#Optimize-contraction-orders"><span>Optimize contraction orders</span></a></li><li><a class="tocitem" href="#Using-the-slicing-technique-to-reduce-the-memory-cost"><span>Using the slicing technique to reduce the memory cost</span></a></li><li><a class="tocitem" href="#Faster-Tropical-tensor-contraction-to-speed-up-MAP-and-MMAP"><span>Faster Tropical tensor contraction to speed up MAP and MMAP</span></a></li><li><a class="tocitem" href="#Working-with-GPUs"><span>Working with GPUs</span></a></li><li><a class="tocitem" href="#Benchmarks"><span>Benchmarks</span></a></li></ul></li><li><span class="tocitem">API</span><ul><li><a class="tocitem" href="../../api/public/">Public</a></li><li><a class="tocitem" href="../../api/internal/">Internal</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Performance tips</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Performance tips</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/TensorBFS/TensorInference.jl/blob/main/docs/src/generated/performance.jl" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Performance-Tips"><a class="docs-heading-anchor" href="#Performance-Tips">Performance Tips</a><a id="Performance-Tips-1"></a><a class="docs-heading-anchor-permalink" href="#Performance-Tips" title="Permalink"></a></h1><h2 id="Optimize-contraction-orders"><a class="docs-heading-anchor" href="#Optimize-contraction-orders">Optimize contraction orders</a><a id="Optimize-contraction-orders-1"></a><a class="docs-heading-anchor-permalink" href="#Optimize-contraction-orders" title="Permalink"></a></h2><p>Let us use a problem instance from the &quot;Promedus&quot; dataset of the UAI 2014 competition as an example.</p><pre><code class="language-julia hljs">using TensorInference
problem = problem_from_artifact(&quot;uai2014&quot;, &quot;MAR&quot;, &quot;Promedus&quot;, 11)
model, evidence = read_model(problem), read_evidence(problem);</code></pre><p>Next, we select the tensor network contraction order optimizer.</p><pre><code class="language-julia hljs">optimizer = TreeSA(ntrials = 1, niters = 5, βs = 0.1:0.3:100)</code></pre><pre><code class="nohighlight hljs">TreeSA{Int64, StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}, GreedyMethod{OMEinsumContractionOrders.MinSpaceOut}, Any}(20, 0.1:0.3:100.0, 1, 5, 1.0, 0.2, :greedy, 0, Any[], GreedyMethod{OMEinsumContractionOrders.MinSpaceOut}(OMEinsumContractionOrders.MinSpaceOut(), 1))</code></pre><p>Here, we choose the local search based <a href="../../api/public/#OMEinsumContractionOrders.TreeSA"><code>TreeSA</code></a> algorithm, which often finds the smallest time/space complexity and supports slicing. One can type <code>?TreeSA</code> in a Julia REPL for more information about how to configure the hyper-parameters of the <a href="../../api/public/#OMEinsumContractionOrders.TreeSA"><code>TreeSA</code></a> method, while the detailed algorithm explanation is in <a href="https://arxiv.org/abs/2108.05665">arXiv: 2108.05665</a>. Alternative tensor network contraction order optimizers include</p><ul><li><a href="../../api/public/#OMEinsumContractionOrders.GreedyMethod"><code>GreedyMethod</code></a> (default, fastest in searching speed but worst in contraction complexity)</li><li><a href="../../api/public/#OMEinsumContractionOrders.KaHyParBipartite"><code>KaHyParBipartite</code></a></li><li><a href="../../api/public/#OMEinsumContractionOrders.SABipartite"><code>SABipartite</code></a></li></ul><pre><code class="language-julia hljs">tn = TensorNetworkModel(model; optimizer, evidence);</code></pre><p>The returned object <code>tn</code> contains a field <code>code</code> that specifies the tensor network with optimized contraction order. To check the contraction complexity, please type</p><pre><code class="language-julia hljs">contraction_complexity(tn)</code></pre><pre><code class="nohighlight hljs">Time complexity (number of element-wise multiplications) = 2^21.71179130602278
Space complexity (number of elements in the largest intermediate tensor) = 2^16.0
Read-write complexity (number of element-wise read and write) = 2^19.152534903357036</code></pre><p>The returned object contains log2 values of the number of multiplications, the number elements in the largest tensor during contraction and the number of read-write operations to tensor elements.</p><pre><code class="language-julia hljs">probability(tn)</code></pre><pre><code class="nohighlight hljs">exp(-19.322038772705987) * fill(1.0)</code></pre><h2 id="Using-the-slicing-technique-to-reduce-the-memory-cost"><a class="docs-heading-anchor" href="#Using-the-slicing-technique-to-reduce-the-memory-cost">Using the slicing technique to reduce the memory cost</a><a id="Using-the-slicing-technique-to-reduce-the-memory-cost-1"></a><a class="docs-heading-anchor-permalink" href="#Using-the-slicing-technique-to-reduce-the-memory-cost" title="Permalink"></a></h2><p>For large scale applications, it is also possible to slice over certain degrees of freedom to reduce the space complexity, i.e. loop and accumulate over certain degrees of freedom so that one can have a smaller tensor network inside the loop due to the removal of these degrees of freedom. In the <a href="../../api/public/#OMEinsumContractionOrders.TreeSA"><code>TreeSA</code></a> optimizer, one can set <code>nslices</code> to a value larger than zero to turn on this feature. As a comparison we slice over 5 degrees of freedom, which can reduce the space complexity by at most 5. In this application, the slicing achieves the largest possible space complexity reduction 5, while the time and read-write complexity are only increased by less than 1, i.e. the peak memory usage is reduced by a factor <span>$32$</span>, while the (theoretical) computing time is increased by at a factor <span>$&lt; 2$</span>.</p><pre><code class="language-julia hljs">optimizer = TreeSA(ntrials = 1, niters = 5, βs = 0.1:0.3:100, nslices=5)
tn = TensorNetworkModel(model; optimizer, evidence);
contraction_complexity(tn)</code></pre><pre><code class="nohighlight hljs">Time complexity (number of element-wise multiplications) = 2^21.802516365121217
Space complexity (number of elements in the largest intermediate tensor) = 2^10.0
Read-write complexity (number of element-wise read and write) = 2^20.597848879637915</code></pre><h2 id="Faster-Tropical-tensor-contraction-to-speed-up-MAP-and-MMAP"><a class="docs-heading-anchor" href="#Faster-Tropical-tensor-contraction-to-speed-up-MAP-and-MMAP">Faster Tropical tensor contraction to speed up MAP and MMAP</a><a id="Faster-Tropical-tensor-contraction-to-speed-up-MAP-and-MMAP-1"></a><a class="docs-heading-anchor-permalink" href="#Faster-Tropical-tensor-contraction-to-speed-up-MAP-and-MMAP" title="Permalink"></a></h2><p>One can enjoy the BLAS level speed provided by <a href="https://github.com/TensorBFS/TropicalGEMM.jl"><code>TropicalGEMM</code></a> by importing the package with</p><pre><code class="language-julia hljs">using TropicalGEMM</code></pre><p>The benchmark in the <code>TropicalGEMM</code> repo shows this performance is close to the theoretical optimal value. Its implementation on GPU is under development in Github repo <a href="https://github.com/ArrogantGao/CuTropicalGEMM.jl"><code>CuTropicalGEMM.jl</code></a> as a part of <a href="https://summer-ospp.ac.cn/">Open Source Promotion Plan summer program</a>.</p><h2 id="Working-with-GPUs"><a class="docs-heading-anchor" href="#Working-with-GPUs">Working with GPUs</a><a id="Working-with-GPUs-1"></a><a class="docs-heading-anchor-permalink" href="#Working-with-GPUs" title="Permalink"></a></h2><p>To upload the computation to GPU, you just add <code>using CUDA</code> before calling the <code>solve</code> function, and set the keyword argument <code>usecuda</code> to <code>true</code>.</p><pre><code class="language-julia hljs">julia&gt; using CUDA
[ Info: OMEinsum loaded the CUDA module successfully

julia&gt; marginals(tn; usecuda = true);</code></pre><p>Functions support <code>usecuda</code> keyword argument includes</p><ul><li><a href="../../api/public/#TensorInference.probability"><code>probability</code></a></li><li><a href="../../api/public/#TensorInference.log_probability"><code>log_probability</code></a></li><li><a href="../../api/public/#TensorInference.marginals"><code>marginals</code></a></li><li><a href="../../api/public/#TensorInference.most_probable_config"><code>most_probable_config</code></a></li></ul><h2 id="Benchmarks"><a class="docs-heading-anchor" href="#Benchmarks">Benchmarks</a><a id="Benchmarks-1"></a><a class="docs-heading-anchor-permalink" href="#Benchmarks" title="Permalink"></a></h2><p>Please check our <a href="generated/">paper (link to be added)</a>.</p><hr/><p><em>This page was generated using <a href="https://github.com/fredrikekre/Literate.jl">Literate.jl</a>.</em></p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../../uai-file-formats/">« UAI file formats</a><a class="docs-footer-nextpage" href="../../api/public/">Public »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.25 on <span class="colophon-date" title="Sunday 23 July 2023 20:05">Sunday 23 July 2023</span>. Using Julia version 1.9.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
